<!DOCTYPE html>
<html lang='en'>

<head>
    <base href="..">
    <link rel="icon" href="assets/favicon.ico" type="image/x-icon"/>
    <link rel="shortcut icon" href="assets/favicon.ico" type="image/x-icon"/>
    <meta property='og:title' content="Categories for Machine Learning">
    <meta property='og:description' content="This seminar series seeks to promote the learning and use of Category Theory by Machine Learning Researchers">
    <meta property='og:image' content="./assets/cat_ai_cat.png">

    <script type="text/javascript" async
        src="/assets/main.js">
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=default">
    </script>
    <link rel="stylesheet" type="text/css" media="all" href="assets/main.css"/>
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <title>Categories for AI</title>
</head>

<body>
    <div class="banner">
        <img src="./assets/cat_ai_cat.png" alt="Conference Template Banner" style="max-width: 1000px;">
        <div class="top-left">
            <span class="title1">Categories</span><span class="title2"> for AI</span> 
        </div>
        <div class="bottom-right">
            Ongoing <br> Virtual Lecture Series
        </div>
    </div>

    <table class="navigation">
        <tr>
            <td class="navigation">
                <a title="Event Home Page" href=".">Home</a>
            </td>
            <td class="navigation">
                <a class="current" title="Event Program" href="program">Program</a> 
            </td>
        </tr>
    </table>
    
    <h2>Program</h2>
    <p>The program consists of <em>2 parts</em>, both consisting of online virtual talks that are streamed on Zoom, recorded, with additional discussions happening on the Zulip chat. <br>
    Subscribe to our <a href="https://calendar.google.com/calendar/ical/952ca4d60c3f40f2c77ca5ebb9421b25bfa49b21e8e679143547378b80f02cfa%40group.calendar.google.com/public/basic.ics">calendar</a>! </p>
        <h3>The Seminars (Ongoing)</h3>
            <p>A more irregular schedule of deep dives into specific topics of Category Theory,
            taught by invited experts in the area, some already showing applications to Machine Learning and some which have
            not been applied yet.</p>
        <h3>Introductory Lectures (Completed)</h3> 
            <p>The lectures are finished for the moment, but you can still check out their <a href="https://www.youtube.com/playlist?list=PLSdFiFTAI4sQ0Rg4BIZcNnU-45I9DI-VB">recordings</a>!<br> 
            We had weekly introductory lectures, where we 
            taught the basics of category theory with a focus on applications to Machine Learning.</p>  
    

    <!-- <p>
    <b>Important:</b><br>
    <ul>
        <li style="list-style: disc;">The full schedule of this page is in the São Paulo, Brazil (GMT -3:00) time zone, which can be quickly converted <a href="https://www.thetimezoneconverter.com/">here</a></li>
        <li style="list-style: disc;">You can add this series of events to <a href="https://calendar.google.com/calendar/u/1?cid=Y19xamJkajQ1dTRycGk4cWF2ajVsZ2VwM25wNEBncm91cC5jYWxlbmRhci5nb29nbGUuY29t">Google Calendar</a> or any <a href="../assets/UnderstandingDL - Lecture Series_c_qjbdj45u4rpi8qavj5lgep3np4@group.calendar.google.com.ics" download="understandingdl">other calendar app</a> you prefer.</li>
        <li style="list-style: disc;">This event <b>does not</b> offer any kind of certificate of attendance!</li>
    </ul>
    </p> -->

    <div style="margin: 5%;"></div>

    
    <h2>Seminars</h2>
    <hr class="divider">

    <h3>Future</h3>

    <table id="tom">
        <tr>
            <td class="date" rowspan="4">
                May 29
            </td>
            <td class="title" style="color: #52739e;">
                Sheaves for AI
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Thomas Gebhart
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                Many data-generating systems studied within machine learning derive global semantics from a collection of complex, local interactions among subsets of the system’s atomic elements.
                In order to properly learn representations of such systems, a machine learning algorithm must have the capacity to faithfully model these local interactions while also ensuring the resulting representations fuse properly into a consistent whole.
                In this talk, we will see that cellular sheaf theory offers an ideal algebro-topological framework for both reasoning about and implementing machine learning models on data which are subject to such local-to-global constraints over a topological space.
                We will introduce cellular sheaves from a categorical perspective before turning to a discussion of sheaf (co)homology as a semi-computable tool for implementing these categorical concepts. Finally, we will observe two practical applications of these ideas in the form of <a href="https://arxiv.org/abs/2012.06333">sheaf neural networks</a>, a generalization of graph neural networks for processing sheaf-valued signals; and <a href="https://arxiv.org/abs/2110.03789">knowledge sheaves</a>, a sheaf-theoretic reformulation of knowledge graph embedding.
            </td>
        </tr>
        <!-- <tr>
            <td>
                <a onclick="toggleText('bio-speaker-7')" >Show / Hide Biography</a>
                <div class='bio' id="bio-speaker-7" style="display: none;">
                    <b>Bio:</b> Jaehoon Lee is a senior research scientist at Google Brain team. His main research focus is on fundamental understanding of deep neural networks; actively working on the infinite-width limit of neural networks and their correspondence to the kernel methods. In 2017, he joined Google and started a research career in machine learning as part of the Google Brain Residency program. Before that he was a postdoctoral fellow at University of British Columbia from 2015-2017 working on theoretical high energy physics. Jaehoon obtained his PhD in physics at the Center for Theoretical Physics, Massachusetts Institute of Technology (MIT) in 2015.
                </div>
            </td>
        </tr> -->
    </table>

    <h3>Past</h3>
    <table id="pietro">
        <tr>
            <td class="date" rowspan="4">
                November 14
            </td>
            <td class="title" style="color: #52739e;">
                Neural network layers as parametric spans - <a href="https://www.youtube.com/watch?v=83a-MwlDy6s">Recording link</a> and <a href="assets/slides/parametric_slides.pdf">Slides</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Pietro Vertechi
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                Properties such as composability and automatic differentiation made artificial neural networks a pervasive tool in applications. Tackling more challenging problems caused neural networks to progressively become more complex and thus difficult to define from a mathematical perspective. In this talk, we will discuss a general definition of linear layer arising from a categorical framework based on the notions of integration theory and parametric spans. This definition generalizes and encompasses classical layers (e.g., dense, convolutional), while guaranteeing existence and computability of the layer's derivatives for backpropagation.
            </td>
        </tr>
        <!-- <tr>
            <td>
                <a onclick="toggleText('bio-speaker-8')" >Show / Hide Biography</a>
                <div class='bio' id="bio-speaker-8" style="display: none;">
                    <b>Bio:</b> Gintare Karolina Dziugaite is a Lead Research Scientist at Element AI, a ServiceNow company. She is also an associate member at Mila, the Quebec AI Institute. Her research combines theoretical and empirical approaches to understanding deep learning, with a focus on generalization and network compression. Before joining Element AI, she obtained her Ph.D. in machine learning from the University of Cambridge, under the supervision of Zoubin Ghahramani. Prior to that, she studied Mathematics at the University of Warwick and read Part III in Mathematics at the University of Cambridge, receiving a Masters of Advanced Study (MASt) in Applied Mathematics. In 2020, Karolina was a member of the Institute for Advanced Study,, participating in the special year on Optimization, Statistics, and Theoretical Machine Learning. In 2019, she was a Simons Fellow during the Foundations of Deep Learning program at the Simons Institute for the Theory of Computing at the University of Berkeley from programs. She was also a long-term participant at the Simons Institute in 2017 and 2020 during programs on theoretical machine learning and interpretable machine learning.
                </div>
            </td>
        </tr> -->
    </table>




    <table id="taco">
        <tr>
            <td class="date" rowspan="4">
                November 21
            </td>
            <td class="title" style="color: #52739e;">
                Causal Model Abstraction & Grounding via Category Theory - <a href="https://www.youtube.com/watch?v=5mZhcXhbciE">Recording link</a> and <a href="assets/slides/2022-11-21-Causality-categories.pdf">Slides</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Taco Cohen
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                Causal models are used in many areas of science to describe data generating processes and reason about the effect of changes to these processes (interventions). Causal models are typically highly abstracted representations of the underlying process, consisting of only a few carefully selected variables, and the causal mechanisms between them. This simplifies causal reasoning, but the relation between the model and the underlying system is never described in mathematical terms, and this has led to considerable philosophical confusions. Furthermore, it has made it hard to understand how causal modeling relates to other fields such as physics (where systems are described by dynamical laws without reference to causes), dynamical systems, and agent-centric frameworks such as Markov Decision Processes (MDPs).<br>
                In this talk we study this idea of abstraction from a categorical perspective, focussing on two questions in particular:
                <ul>
                    <li>What is an appropriate notion of morphism between causal models? When can we say that one model is an abstraction of another? How can we set up a convenient category of causal models?</li>
                    <li>What does it mean for a causal model to be an abstraction of an underlying dynamical system or Markov decision process?</li>
                </ul>
                To answer the first question we will mainly survey the existing literature, while for the second we will present a new approach to grounding causal models in dynamical systems and MDPs via natural transformations, and giving for the first time a mathematical definition of "causal mechanism" as a functional relationship between outcome variables that is invariant to interventions (modelled as transformations of the state space).
            </td>
        </tr>
        <!-- <tr>
            <td>
                <a onclick="toggleText('bio-speaker-7')" >Show / Hide Biography</a>
                <div class='bio' id="bio-speaker-7" style="display: none;">
                    <b>Bio:</b> Jaehoon Lee is a senior research scientist at Google Brain team. His main research focus is on fundamental understanding of deep neural networks; actively working on the infinite-width limit of neural networks and their correspondence to the kernel methods. In 2017, he joined Google and started a research career in machine learning as part of the Google Brain Residency program. Before that he was a postdoctoral fellow at University of British Columbia from 2015-2017 working on theoretical high energy physics. Jaehoon obtained his PhD in physics at the Center for Theoretical Physics, Massachusetts Institute of Technology (MIT) in 2015.
                </div>
            </td>
        </tr> -->
    </table>




    <table id="tai-danae">
        <tr>
            <td class="date" rowspan="4">
                December 12
            </td>
            <td class="title" style="color: #52739e;">
                Category Theory Inspired by LLMs - <a href="https://www.youtube.com/watch?v=_LgWD3UTKfw">Recording link</a> and <a href="assets/slides/TDB_slides.pdf">Slides</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Tai-Danae Bradley
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                The success of today's large language models (LLMs) is striking, especially given that the training data consists of raw, unstructured text. In this talk, we'll see that category theory can provide a natural framework for investigating this passage from texts—and probability distributions on them—to a more semantically meaningful space. To motivate the mathematics involved, we will open with a basic, yet curious, analogy between linear algebra and category theory. We will then define a category of expressions in language enriched over the unit interval and afterwards pass to enriched copresheaves on that category. We will see that the latter setting has rich mathematical structure and comes with ready-made tools to begin exploring that structure.
            </td>
        </tr>
        <!-- <tr>
            <td>
                <a onclick="toggleText('bio-speaker-6')" >Show / Hide Biography</a>
                <div class='bio' id="bio-speaker-6" style="display: none;">
                    <b>Bio:</b> Lechao Xiao is a research scientist in the Brain Team, Google Research.  He works on theory of deep learning, including optimization, Gaussian Processes, generalization of neural networks. Before joining Google, he was a Hans Rademacher Instructor at the University of Pennsylvania working on Harmonic analysis. He received his Ph.D. in mathematics from the University of Illinois at Urbana-Champaign in 2014 and his B.S. in mathematics from Zhejiang University in 2009.
                </div>
            </td>
        </tr> -->
    </table>

    <table id="jules">
        <tr>
            <td class="date" rowspan="4">
                March 20
            </td>
            <td class="title" style="color: #52739e;">
                Introduction to Categorical Cybernetics - <a href="https://www.youtube.com/watch?v=_EShi5y3b0E">Recording link</a> and <a href="assets/slides/CyberCat-talk.pdf">Slides</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Jules Hedge
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                Categorical cybernetics is based on two things: (1) the abstract theory of categories of optics and related things, and (2) a whole bunch of specific examples.
                These tend to arise in topics that historically were called "cybernetics" (before that term drifted beyond recognition) - AI, control theory, game theory, systems theory.
                Specific examples of "things that compose optically" are derivatives (well known as backprop), exact and approximate Bayesian inverses, payoffs in game theory, values in control theory and reinforcement learning, updates of data (the original setting for lenses), and updates of state machines.
                I'll do a gentle tour through these, emphasising their shared structure and the field we're developing to study it. <br>
                The talk will cover material related to the paper <a href=https://arxiv.org/abs/2105.06332>Towards Foundations of Categorical Cybernetics</a>
            </td>
        </tr>
    </table>

    <table id="spivak">
        <tr>
            <td class="date" rowspan="4">
                March 27
            </td>
            <td class="title" style="color: #52739e;">
                Dynamic organizational systems: from deep learning to prediction markets - <a href="https://www.youtube.com/watch?v=Z5fdB6aUNBw">Recording link</a> and <a href="assets/slides/spivak.pdf">Slides</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                David Spivak
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                In training artificial neural networks (ANNs), both neurons and arbitrary populations of neurons can be seen to perform the same type of task. Indeed, at any given moment they provide a function A-->B, and given any input from A and loss signal on B, they do two things: provide an updated function A-->B and backpropagate a loss signal on A. Populations of neurons, which we called "Learners", can be put together in series or in parallel, forming a symmetric monoidal category. However, ANNs satisfy an additional property: there is a consistent method by which the functions update and errors backpropagate; namely, they all use gradient descent. The chain rule implies that the composite of gradient descenders is again a gradient descender.<br> 

                In this talk I will discuss a generalization called "dynamic organizational systems", which includes ANNs, prediction markets, Hebbian learning, and strategic games. It is founded on the category Poly of polynomial functors, which generalizes Lens. I will review the relevant background on Poly and then explain dynamic organizational systems as coherent procedures by which a network of component systems can rewire its network structure in response to the data flowing through it. I'll explain the ANN case, and possibly the prediction market case, time permitting.<br>
                The talk will cover material related to the paper <a href="https://arxiv.org/abs/2205.03906">Dynamic categories, dynamic operads: From deep learning to prediction markets</a>

            </td>
        </tr>
    </table>



    <h2>Lecture Series</h2>
    <b>Week 1</b>
    <hr class="divider">
    
    <table id="bruno">
        <tr>
            <td class="date" rowspan="4">
               Week of October 10
            </td>
            <td class="title" style="color: #52739e;">
                Week 1: Why Category Theory? - <a href="https://www.youtube.com/watch?v=4poHENv4kR0">Recording link</a> and <a href="https://docs.google.com/presentation/d/1e2RV1AOAa7rG320A2fSYtoitvVDUzqeh7PINIIJY_ss/edit?resourcekey=0-Nti559bQUftJgUNKqXMwYw#slide=id.g1399aa18765_0_18">Slides</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Bruno Gavranović
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                By the end of this week you will:
                <ul>
                    <li>Get a sense of the philosophy and motivation behind Category Theory</li>
                    <li>Learn about the recent wave of its applications emerging throughout the sciences</li>
                    <li>Understand how this formal mathematical language rigorously adheres to the concept of modularity</li>
                    <li>Dispel with the fallacy that CT is not relevant to practical disciplines such as programming or engineering</li>
                    <li>Get a sense of how CT can help us design and scale our deep learning systems</li>
                </ul>

            </td>
        </tr>
        <!-- <tr>
            <td>
                <a onclick="toggleText('bio-speaker-1')" >Show / Hide Biography</a>
                <div class='bio' id="bio-speaker-1" style="display: none;">
                    <b>Bio:</b> Boaz Barak is the Gordon McKay professor of Computer Science at Harvard University's John A. Paulson school of Engineering and Applied Sciences. His research interests include all areas of theoretical computer science and in particular cryptography and computational complexity. Previously, he was a principal researcher at Microsoft Research New England, and before that an associate professor (with tenure) at Princeton University's computer science department. Barak has won the ACM dissertation award, the Packard and Sloan fellowships, and was also selected for Foreign Policy magazine's list of 100 leading global thinkers for 2014 and chosen as a Simons investigator in 2017 . He serves on the editorial boards of several journals and is also a member of the Committee for the Advancement of Theoretical Computer Science and the scientific advisory board for the Simons Institute for the Theory of Computing. He wrote with Sanjeev Arora the textbook "Computational Complexity: A Modern Approach".
                    <br><br>
                    Gal Kaplun is a third-year Ph.D. candidate at the Computer Science Department at Harvard University, under the supervision of Prof. Yaron Singer. Gal is working with the Harvard Theory of Machine Learning group, focusing on a deep understanding of Machine Learning models.

                    Gal's research interests revolve around investigating the mysteries of Deep Learning---why and how overparameterized models generalize, what are the failure modes of Deep Networks, how can we make models robust to distribution shift and adversarial examples. At the moment, Gal is exploring the theory behind the emergent area of self-supervised learning, in particular, what is the driving mechanism behind contrastive learning

                </div>
            </td>
        </tr> -->
    </table>
    

    <b>Week 2</b>
    <hr class="divider">
    
    <table id="petar">
        <tr>
            <td class="date" rowspan="4">
                Week of October 17
            </td>
            <td class="title" style="color: #52739e;">
                Week 2: Essential building blocks: Categories and Functors - <a href="https://www.youtube.com/watch?v=jU7KyZn_hBc">Recording link</a> and <a href="https://docs.google.com/presentation/d/1z8QmCWsImykggqrHt6pGoQ2pW1_qfHwdX1BnqqpnCdM/edit#slide=id.g1399aa18765_0_18">Slides</a>
                 <!-- - <a href="">Recording link</a> -->
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Petar Veličković
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                By the end of this week you will:
                <ul>
                   <li>Understand the key building blocks of categories: objects, morphisms and functors.</li>
                   <li>Leverage these concepts to explain several standard mathematical constructs: sets, relations, and groups.</li>
                   <li>Get comfortable manipulating these concepts through several worked exercises.</li>
                   <li>Ground all of the above in relevant deep learning context, with links to functional programming.</li>
                   <li>Show how we can build an effective "type checker" for deep learning using the category of sets.</li>
                </ul>
                These lectures will help explain key parts of <a href="https://arxiv.org/abs/2203.15544">Graph Neural Networks are Dynamic Programmers</a> (NeurIPS 2022)
            </td>
         </tr>
        <!-- <tr>
            <td>
                <a onclick="toggleText('bio-speaker-2')" >Show / Hide Biography</a>
                <div class='bio' id="bio-speaker-2" style="display: none;">
                    <b>Bio:</b> Jascha is a senior staff research scientist in Google Brain, and leads a research team with interests spanning machine learning, physics, and neuroscience. He was previously a visiting scholar in Surya Ganguli's lab at Stanford, and an academic resident at Khan Academy. He earned his PhD in 2012 in Bruno Olshausen's lab in the Redwood Center for Theoretical Neuroscience at UC Berkeley. Prior to his PhD, he spent several years working for NASA on the Mars Exploration Rover mission.

                </div>
            </td>
        </tr> -->
    </table>

    <b>Week 3</b>
    <hr class="divider">
    
    <table id="bruno">
        <tr>
            <td class="date" rowspan="4">
                Week of October 24
            </td>
            <td class="title" style="color: #52739e;">
                Week 3: Categorical Dataflow: Optics and Lenses as data structures for backpropagation - <a href="https://www.youtube.com/watch?v=p4iRU4pBkCo">Recording link</a> and <a href="assets/slides/MonoidalCatsLensesOptics.pdf">Slides</a>
                <!-- - <a href="">Recording link</a> -->
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Bruno Gavranović
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                By the end of this week you will:
                <ul>
                    <li>Understand the difference between a monoidal and a cartesian category</li>
                    <li>Get comfortable using their formal graphical language: string diagrams </li>
                    <li>Learn about lenses and optics, abstract interfaces for modelling bidirectional data flow</li>
                    <li>See examples of lenses and optics modelling backpropagation, gradient descent, value iteration and more</li>
                    <li>Understand how the chain rule is a special case of lens composition</li>
                </ul>
                These lectures will help explain key parts of <a href="https://arxiv.org/abs/2103.01931">Categorical Foundations of Gradient-Based Learning</a> (ESOP 2022)
            </td>
        </tr>
        <!-- <tr>
            <td>
                <a onclick="toggleText('bio-speaker-ari')" >Show / Hide Biography</a>
                <div class='bio' id="bio-speaker-ari" style="display: none;">
                    <b>Bio:</b> Why do large learning rates often produce better results? Why do “infinitely wide” networks trained using kernel methods tend to underperform ordinary networks? In the talk I will argue that these questions are related. Existing kernel-based theory can explain the dynamics of networks trained with small learning rates. However, optimal performance is often achieved at large learning rates, where we find qualitatively different dynamics that converge to flat minima. The distinction between the small and large learning rate phases becomes sharp at infinite width, and is reminiscent of nonperturbative phase transitions that appear in physical systems.
                </div>
            </td>
        </tr> -->
    </table>

    <b>Week 4</b>
    <hr class="divider">

    
    <table id="pim">
        <tr>
            <td class="date" rowspan="4">
                Week of October 31
            </td>
            <td class="title" style="color: #52739e;">
                Week 4: Geometric Deep Learning & Naturality - <a href="https://www.youtube.com/watch?v=2tPcSYLBnxg">Recording link</a> and <a href="https://drive.google.com/file/d/1xo4B8QaHeOmCwdCHeUpxQrZVuNyjaiSI/view">Slides</a>
                <!-- - <a href="">Recording link</a> -->
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Pim de Haan
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                By the end of this week you will:
                <ul>
                    <li>Understand the role of Symmetry equivariance in geometric deep learning</li>
                    <li>Learn about Natural transformations between functors as a generalization of equivariant transformations between group representations</li>
                    <li>Be able to build more expressive graph networks via naturality</li>
                </ul>
                These lectures will help explain key parts of <a href="https://arxiv.org/abs/2007.08349">Natural Graph Networks</a> (NeurIPS 2020)
            </td>
        </tr>
        <!-- <tr>
            <td>
                <a onclick="toggleText('bio-speaker-Sam')" >Show / Hide Biography</a>
                <div class='bio' id="bio-speaker-Sam" style="display: none;">
                    <b>Bio:</b> Sam is a Senior Research Scientist at Google Brain working at the intersection between Machine Learning and Physics. His work focuses on better understanding the large-width limit of neural networks using techniques from statistical physics as well as applying advances in Machine Learning to physical systems. Sam received his PhD in Physics from the University of Pennsylvania in 2015.

                </div>
            </td>
        </tr> -->
    </table>

    <b>Week 5</b>
    <hr class="divider">

    
    <table id="andrew">
        <tr>
            <td class="date" rowspan="4">
                Week of November 7
            </td>
            <td class="title" style="color: #52739e;">
                Week 5: Monoids, Monads, Mappings, and lstMs - <a href="https://www.youtube.com/watch?v=y16JDvRi8GU">Recording link</a> and <a href="https://docs.google.com/presentation/d/1TBEaz-S5Zq42nDlkJVxDUBIyJ16rK9sD/edit#slide=id.p1">Slides</a>
                <!-- - <a href="">Recording link</a> -->
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Andrew Dudzik
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                By the end of this week you will:
                <ul>
                    <li>Know several useful basic monads</li>
                    <li>Be familiar with different equivalent descriptions of monads, including the Kleisli and Eilenberg-Moore categories</li>
                    <li>Understand how monoids formalize recurrence and aggregation</li>
                    <li> Finally laugh at the in-joke about monads and monoids</li>
                </ul>
                These lectures will help explain key parts of <a href="https://arxiv.org/abs/2203.15544">Graph Neural Networks are Dynamic Programmers</a> (NeurIPS 2022)
            </td>
        </tr>
        <!-- <tr>
            <td>
                <a onclick="toggleText('bio-speaker-5')" >Show / Hide Biography</a>
                <div class='bio' id="bio-speaker-5" style="display: none;">
                    <b>Bio:</b> Stanislav Fort is a PhD student at Stanford University, advised by Prof Surya Ganguli. His research focuses on developing a scientific understanding of deep learning and on applications of machine learning and artificial intelligence in the physical sciences, in domains spanning from X-ray astrophysics to quantum computing. Stanislav spent a year as a Google AI Resident, where he worked on deep learning theories and their applications in collaboration with colleagues from Google Brain and DeepMind. He received his Bachelor’s and Master’s degrees in Physics at Trinity College, University of Cambridge, and a Master’s degree at Stanford University.
                </div>
            </td>
        </tr> -->
    </table>


    <!-- <b>Week 10</b>
    <hr class="divider">
    <table>
        <tr>
            <td class="date" rowspan="3">
                Jun 21, 2021 <br> 04:00 PM (GMT -3:00)
            </td>
            <td class="title" style="color: #b2132e;">
                Introductory lecture 10: Introduction to Category Theory: Up to Monoidal Categories (Portuguese) - <a href="https://www.youtube.com/watch?v=7ypPI7jHOTA">Recording link</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Jose Vitor Paiva Miranda Siqueira (University of Cambridge)
            </td>
        </tr> -->
        <!-- <tr>
            <td class="abstract">
                Here&rsquo;s where you put the abstract of the talk. Notice that with MathJax you can include mathematical notation in the abstract like \(f(x)=3x-7\) or like \(h^0(X_t,\omega_{X_t}^{\otimes m})\) or whatever you want. Just notice the MathJax line in the head of this HTML document that allows this. I&rsquo;m going to blab just a tiny bit more in this abstract, but all the other abstracts below, except the ones for the plenary talks, will be just <i>Lorem Ipsum</i> text that needs to be replaced by real abstracts.
            </td>
        </tr> -->
    <!-- </table>
    
    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="4">
                
                Jun 25, 2021 <br> 04:00 PM (GMT -3:00)

            </td>
            <td class="title" style="color: #52739e;">
                Lecture 10: Backprop as a Functor (English) - <a href="https://www.youtube.com/watch?v=N9zZeACcV98">Recording link</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Brendan Fong (MIT / Topos Institute)
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                A supervised learning algorithm searches over a set of functions \(A→B\) parametrised by a space \(P\) to find the best approximation to some ideal function \(f:A→B\). It does this by taking examples \((a,f(a))∈A×B\), and updating the parameter according to some rule. We define a category where these update rules may be composed, and show that gradient descent --- with respect to a fixed step size and an error function satisfying a certain property --- defines a monoidal functor from a category of parametrised functions to this category of update rules. This provides a structural perspective on backpropagation, as well as a broad generalisation of neural networks.
            </td>
        </tr>
        <tr>
            <td>
                <a onclick="toggleText('bio-speaker-10')" >Show / Hide Biography</a>
                <div class='bio' id="bio-speaker-10" style="display: none;">
                    <b>Bio:</b> Brendan Fong oversees coordination and strategic planning at Topos. He holds a PhD in computer science from Oxford, and undertook postdoctoral studies in systems engineering and mathematics at UPenn and MIT. He is a founding executive editor of the open-access journal Compositionality, and co-authored the textbook An Invitation to Applied Category Theory. Brendan believes technologies of connection and integration are already transforming the world we live in, and is dedicated to ensuring that these transformations benefit society-at-large.
                </div>
            </td>
        </tr>
    </table>


    <b>Week 11</b>
    <hr class="divider"> -->
    <!-- <table>
        <tr>
            <td class="date" rowspan="3">
                Jun 28, 2021 <br> Time yet to be defined
            </td>
            <td class="title" style="color: #b2132e;">
                Introductory lecture 11: (Portuguese)
            </td>
        </tr>
        <tr>
            <td class="speaker">
                To be defined
            </td>
        </tr>

    </table> -->
    
    <!-- <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="4">
                
                Jul 2, 2021 <br> 02:00 PM (GMT -3:00)

            </td>
            <td class="title" style="color: #52739e;">
                Lecture 11: Learning Functors using Gradient Descent (English) - <a href="https://www.youtube.com/watch?v=SQqzs3uAqGM">Recording link</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Bruno Gavranović (University of Strathclyde)
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                CycleGAN is a general approach to unpaired image-to-image translation that has been getting attention in recent years. Inspired by categorical database systems, we show that CycleGAN is a "schema", i.e. a specific category presented by generators and relations, whose specific parameter instantiations are just set-valued functors on this schema. We show that enforcing cycle-consistencies amounts to enforcing composition invariants in this category. We generalize the learning procedure to arbitrary such categories and show that a special class of functors, rather than functions, can be learned using gradient descent. Using this framework we design a novel neural network system capable of learning to insert and delete objects from images without paired data. We qualitatively evaluate the system on three different datasets and obtain promising results.

            </td>
        </tr>
        <tr>
            <td>
                <a onclick="toggleText('bio-speaker-11')" >Show / Hide Biography</a>
                <div class='bio' id="bio-speaker-11" style="display: none;">
                    <b>Bio:</b> Bruno Gavranović is a PhD student at the Mathematically Structured Programming (MSP) group in Glasgow, advised by Neil Ghani. He is  understanding machine learning and game theory through the lens of category theory and functional programming. Before joining MSP, he obtained his Bachelors and Masters at Faculty of Electrical Engineering and Computing (FER) in Zagreb.
                </div>
            </td>
        </tr>
    </table>


    <table class="plenary">
        <tr>
            <td class="date" rowspan="3">
                Jul 09, 2021 <br> 11:00 AM (GMT -3:00)
            </td>
            <td class="title" style="color: #52739e">
                Panel: The Many Paths to Understanding Deep Learning (English) - <a href="https://www.youtube.com/watch?v=NfvYufQwA_o">Recording link link</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Brendan Fong (MIT / Topos Institute), Gintare Karolina (Element AI), Oriol Vinyals (DeepMind), Yasaman Bahri (Google Research)
            </td>
        </tr>
        <tr> -->
            <!-- <td class="abstract">
                Here&rsquo;s the abstract for the plenary talk! Notice again that the formatting of this time-block is a bit different that the rest of of the talks.
                Filler text: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum dignissim et est et euismod. Fusce et metus tempus, pellentesque ex at, convallis nulla. Ut fringilla commodo tincidunt. Fusce sed est eu massa placerat iaculis eu at mauris. Nullam ut mollis nisi, quis malesuada risus. Interdum et malesuada fames ac ante ipsum primis in faucibus. Nam ipsum tortor, suscipit non tincidunt vel, bibendum in libero. Nulla facilisi. Pellentesque vitae neque metus. Cras quis est pharetra, vestibulum nisl et, viverra ipsum. Etiam porta dignissim purus, quis tempor metus volutpat eu. Praesent pulvinar libero eget purus tincidunt finibus.
            </td> -->
        <!-- </tr>
    </table> -->

    <table class="footer">
        <tr>
            <td class="footer">Design by <a target="_blank" href="http://math.ucr.edu/~mpierce/">Mike Pierce</a></td> 
            <!-- <td class="footer">&copy; Data ICMC</td>  -->
        </tr>
    </table>


</body>

</html>


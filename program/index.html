<!DOCTYPE html>
<html lang='en'>

<head>
    <base href="..">
    <link rel="shortcut icon" type="image/png" href="assets/favicon.png"/>
    <meta property='og:title' content="Categories for Machine Learning">
    <meta property='og:description' content="This seminar series seeks to promote 
                                    the learning and use of Category Theory by Machine Learning Researchers">
    <meta property='og:image' content="./assets/ICMC (1).jpg">

    <script type="text/javascript" async
        src="/assets/main.js">
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=default">
    </script>
    <link rel="stylesheet" type="text/css" media="all" href="assets/main.css"/>
    <meta name="description" content="A Data ICMC Lecture Series">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <title>Categories for AI</title>
</head>

<body>
    <div class="banner">
        <img src="../assets/ICMC (1).jpg" alt="Conference Template Banner" style="max-width: 1000px;">
        <div class="top-left">
            <span class="title1">Categories</span><span class="title2"> for AI</span> 
        </div>
        <div class="bottom-right">
            October, 2022 <br> Virtual Lecture Series
        </div>
    </div>

    <table class="navigation">
        <tr>
            <td class="navigation">
                <a title="Event Home Page" href=".">Home</a>
            </td>
            <td class="navigation">
                <a class="current" title="Event Program" href="program">Program</a> 
            </td>
        </tr>
    </table>

    <h2 class="forms">Sign up for the course via this <a href="https://forms.gle/ovW8MTtctwfEcLmG7">this form</a>!</h2>
    
    <h2>Program</h2>
    <p>This lecture series consists of <b>2 parts</b> , 
        these being: the <b>introductory lectures</b> and the <b>seminars</b>. 
            During the first part we'll have 1-2 introductory lectures per week, where we 
            will teach the basics of category theory with a focus on applications to Machine Learning.  
            <br>The seminars will be deep dives into specific topics of Category Theory,
            some already showing applications to Machine Learning and some which have
            not beeen applied yet.
            <br>Also, during both parts we'll have discussions and extra activities in our Zulip stream!
    </p>

    <!-- <p>
    <b>Important:</b><br>
    <ul>
        <li style="list-style: disc;">The full schedule of this page is in the São Paulo, Brazil (GMT -3:00) time zone, which can be quickly converted <a href="https://www.thetimezoneconverter.com/">here</a></li>
        <li style="list-style: disc;">You can add this series of events to <a href="https://calendar.google.com/calendar/u/1?cid=Y19xamJkajQ1dTRycGk4cWF2ajVsZ2VwM25wNEBncm91cC5jYWxlbmRhci5nb29nbGUuY29t">Google Calendar</a> or any <a href="../assets/UnderstandingDL - Lecture Series_c_qjbdj45u4rpi8qavj5lgep3np4@group.calendar.google.com.ics" download="understandingdl">other calendar app</a> you prefer.</li>
        <li style="list-style: disc;">This event <b>does not</b> offer any kind of certificate of attendance!</li>
    </ul>
    </p> -->

    <div style="margin: 5%;"></div>

    
    <b>Week 1</b>
    <hr class="divider">
    
    <table id="bruno">
        <tr>
            <td class="date" rowspan="4">
               Week of October 10
            </td>
            <td class="title" style="color: #52739e;">
                Week 1: Why Category Theory?
                 <!-- - <a href="">Recording link</a> -->
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Bruno Gavranović
            </td>
        </tr>
        
        <tr>
            <td class="learning-goals">
                By the end of this week you will:
                <ul>
                    <li>Get a sense of the philosophy and motivation behind Category Theory</li>
                    <li>Learn about the recent wave of its applications emerging throughout the sciences</li>
                    <li>Understand how this formal mathematical language rigorously adheres to the concept of modularity</li>
                    <li>Dispel with the fallacy that CT is not relevant to practical disciplines such as programming or engineering</li>
                    <li>Get a sense of how CT can help us design and scale our deep learning systems</li>
                </ul>

            </td>
        </tr>
        <!-- <tr>
            <td>
                <a onclick="toggleText('bio-speaker-1')" >Show / Hide Biography</a>
                <div class='bio' id="bio-speaker-1" style="display: none;">
                    <b>Bio:</b> Boaz Barak is the Gordon McKay professor of Computer Science at Harvard University's John A. Paulson school of Engineering and Applied Sciences. His research interests include all areas of theoretical computer science and in particular cryptography and computational complexity. Previously, he was a principal researcher at Microsoft Research New England, and before that an associate professor (with tenure) at Princeton University's computer science department. Barak has won the ACM dissertation award, the Packard and Sloan fellowships, and was also selected for Foreign Policy magazine's list of 100 leading global thinkers for 2014 and chosen as a Simons investigator in 2017 . He serves on the editorial boards of several journals and is also a member of the Committee for the Advancement of Theoretical Computer Science and the scientific advisory board for the Simons Institute for the Theory of Computing. He wrote with Sanjeev Arora the textbook "Computational Complexity: A Modern Approach".
                    <br><br>
                    Gal Kaplun is a third-year Ph.D. candidate at the Computer Science Department at Harvard University, under the supervision of Prof. Yaron Singer. Gal is working with the Harvard Theory of Machine Learning group, focusing on a deep understanding of Machine Learning models.

                    Gal's research interests revolve around investigating the mysteries of Deep Learning---why and how overparameterized models generalize, what are the failure modes of Deep Networks, how can we make models robust to distribution shift and adversarial examples. At the moment, Gal is exploring the theory behind the emergent area of self-supervised learning, in particular, what is the driving mechanism behind contrastive learning

                </div>
            </td>
        </tr> -->
    </table>
    

    <b>Week 2</b>
    <hr class="divider">
    
    <table id="petar">
        <tr>
            <td class="date" rowspan="4">
                Week of October 17
            </td>
            <td class="title" style="color: #52739e;">
                Week 2: Essential building blocks: Categories and Functors
                 <!-- - <a href="">Recording link</a> -->
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Petar Veličković
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                By the end of this week you will:
                <ul>
                   <li>Understand the key building blocks of categories: objects, morphisms and functors.</li>
                   <li>Leverage these concepts to explain several standard mathematical constructs: sets, relations, and groups.</li>
                   <li>Get comfortable manipulating these concepts through several worked exercises.</li>
                   <li>Ground all of the above in relevant deep learning context, with links to functional programming.</li>
                   <li>Show how we can build an effective "type checker" for deep learning using the category of sets.</li>
                </ul>
                These lectures will help explain key parts of <a href="https://arxiv.org/abs/2203.15544">Graph Neural Networks are Dynamic Programmers</a> (ICLR Workshop on Geometrical and Topological Representation Learning 2022)
            </td>
         </tr>
        <!-- <tr>
            <td>
                <a onclick="toggleText('bio-speaker-2')" >Show / Hide Biography</a>
                <div class='bio' id="bio-speaker-2" style="display: none;">
                    <b>Bio:</b> Jascha is a senior staff research scientist in Google Brain, and leads a research team with interests spanning machine learning, physics, and neuroscience. He was previously a visiting scholar in Surya Ganguli's lab at Stanford, and an academic resident at Khan Academy. He earned his PhD in 2012 in Bruno Olshausen's lab in the Redwood Center for Theoretical Neuroscience at UC Berkeley. Prior to his PhD, he spent several years working for NASA on the Mars Exploration Rover mission.

                </div>
            </td>
        </tr> -->
    </table>

    <b>Week 3</b>
    <hr class="divider">
    
    <table id="bruno">
        <tr>
            <td class="date" rowspan="4">
                Week of October 24
            </td>
            <td class="title" style="color: #52739e;">
                Week 3: Categorical Dataflow: Optics and Lenses as data structures for backpropagation 
                <!-- - <a href="">Recording link</a> -->
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Bruno Gavranović
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                By the end of this week you will:
                <ul>
                    <li>Understand the difference between a monoidal and a cartesian category</li>
                    <li>Get comfortable using their formal graphical language: string diagrams </li>
                    <li>Learn about lenses and optics, abstract interfaces for modelling bidirectional data flow</li>
                    <li>See examples of lenses and optics modelling backpropagation, gradient descent, value iteration and more</li>
                    <li>Understand how the chain rule is a special case of lens composition</li>
                </ul>
                These lectures will help explain key parts of <a href="https://arxiv.org/abs/2103.01931">Categorical Foundations of Gradient-Based Learning</a> (ESOP 2022)
            </td>
        </tr>
        <!-- <tr>
            <td>
                <a onclick="toggleText('bio-speaker-ari')" >Show / Hide Biography</a>
                <div class='bio' id="bio-speaker-ari" style="display: none;">
                    <b>Bio:</b> Why do large learning rates often produce better results? Why do “infinitely wide” networks trained using kernel methods tend to underperform ordinary networks? In the talk I will argue that these questions are related. Existing kernel-based theory can explain the dynamics of networks trained with small learning rates. However, optimal performance is often achieved at large learning rates, where we find qualitatively different dynamics that converge to flat minima. The distinction between the small and large learning rate phases becomes sharp at infinite width, and is reminiscent of nonperturbative phase transitions that appear in physical systems.
                </div>
            </td>
        </tr> -->
    </table>

    <b>Week 4</b>
    <hr class="divider">

    
    <table id="pim">
        <tr>
            <td class="date" rowspan="4">
                Week of October 31
            </td>
            <td class="title" style="color: #52739e;">
                Week 4: Geometric Deep Learning & Naturality 
                <!-- - <a href="">Recording link</a> -->
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Pim de Haan
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                By the end of this week you will:
                <ul>
                    <li>Understand the role of Symmetry equivariance in geometric deep learning</li>
                    <li>Learn about Natural transformations between functors as a generalization of equivariant transformations between group representations</li>
                    <li>Be able to build more expressive graph networks via naturality</li>
                </ul>
                These lectures will help explain key parts of <a href="https://arxiv.org/abs/2007.08349">Natural Graph Networks</a> (NeurIPS 2020)
            </td>
        </tr>
        <!-- <tr>
            <td>
                <a onclick="toggleText('bio-speaker-Sam')" >Show / Hide Biography</a>
                <div class='bio' id="bio-speaker-Sam" style="display: none;">
                    <b>Bio:</b> Sam is a Senior Research Scientist at Google Brain working at the intersection between Machine Learning and Physics. His work focuses on better understanding the large-width limit of neural networks using techniques from statistical physics as well as applying advances in Machine Learning to physical systems. Sam received his PhD in Physics from the University of Pennsylvania in 2015.

                </div>
            </td>
        </tr> -->
    </table>

    <b>Week 5</b>
    <hr class="divider">

    
    <table id="andrew">
        <tr>
            <td class="date" rowspan="4">
                Week of November 7
            </td>
            <td class="title" style="color: #52739e;">
                Week 5: Monoids, Monads, Mappings, and lstMs 
                <!-- - <a href="">Recording link</a> -->
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Andrew Dudzik
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                By the end of this week you will:
                <ul>
                    <li>Know several useful basic monads</li>
                    <li>Be familiar with different equivalent descriptions of monads, including the Kleisli and Eilenberg-Moore categories</li>
                    <li>Understand how monoids formalize recurrence and aggregation</li>
                    <li> Finally laugh at the in-joke about monads and monoids</li>
                </ul>
                These lectures will help explain key parts of <a href="https://arxiv.org/abs/2203.15544">Graph Neural Networks are Dynamic Programmers</a> (NeurIPS 2022)
            </td>
        </tr>
        <!-- <tr>
            <td>
                <a onclick="toggleText('bio-speaker-5')" >Show / Hide Biography</a>
                <div class='bio' id="bio-speaker-5" style="display: none;">
                    <b>Bio:</b> Stanislav Fort is a PhD student at Stanford University, advised by Prof Surya Ganguli. His research focuses on developing a scientific understanding of deep learning and on applications of machine learning and artificial intelligence in the physical sciences, in domains spanning from X-ray astrophysics to quantum computing. Stanislav spent a year as a Google AI Resident, where he worked on deep learning theories and their applications in collaboration with colleagues from Google Brain and DeepMind. He received his Bachelor’s and Master’s degrees in Physics at Trinity College, University of Cambridge, and a Master’s degree at Stanford University.
                </div>
            </td>
        </tr> -->
    </table>


    <!-- <b>Week 6</b>
    <hr class="divider">

    
    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="4">
                May 28, 2021 <br> 04:00 PM (GMT -3:00)
            </td>
            <td class="title" style="color: #52739e;">
                Lecture 6: Disentangling Trainability and Generalization in Deep Neural Networks (English) - <a href="https://www.youtube.com/watch?v=4wiFeJOdro4">Recording link</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Lechao Xiao (Google Brain)
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                A longstanding goal in the theory of deep learning is to characterize the conditions under which a given neural network architecture will be trainable, and if so, how well it might generalize to unseen data. In this work, we provide such a characterization in the limit of very wide and very deep networks, for which the analysis simplifies considerably. For wide networks, the trajectory under gradient descent is governed by the Neural Tangent Kernel (NTK), and for deep networks, the NTK itself maintains only weak data dependence. By analyzing the spectrum of the NTK, we formulate necessary conditions for trainability and generalization across a range of architectures, including Fully Connected Networks (FCNs) and Convolutional Neural Networks (CNNs). We identify large regions of hyperparameter space for which networks can memorize the training set but completely fail to generalize. We find that CNNs without global average pooling behave almost identically to FCNs, but that CNNs with pooling have markedly different and often better generalization performance. A thorough empirical investigation of these theoretical results shows excellent agreement on real datasets.
            </td>
        </tr>
        <tr>
            <td>
                <a onclick="toggleText('bio-speaker-6')" >Show / Hide Biography</a>
                <div class='bio' id="bio-speaker-6" style="display: none;">
                    <b>Bio:</b> Lechao Xiao is a research scientist in the Brain Team, Google Research.  He works on theory of deep learning, including optimization, Gaussian Processes, generalization of neural networks. Before joining Google, he was a Hans Rademacher Instructor at the University of Pennsylvania working on Harmonic analysis. He received his Ph.D. in mathematics from the University of Illinois at Urbana-Champaign in 2014 and his B.S. in mathematics from Zhejiang University in 2009.
                </div>
            </td>
        </tr>
    </table> -->


    <!-- <b>Week 7</b>
    <hr class="divider">
    <table>
        <tr>
            <td class="date" rowspan="3">
                May 31, 2021 <br> 06:00 PM (GMT -3:00)
            </td>
            <td class="title" style="color: #b2132e;">
                Introductory lecture 7: Introduction to Statistical Mechanics (Portuguese) - <a href="https://youtu.be/LoYyQ0M7j5Y">Recording link</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Tereza Cristina da Rocha Mendes (Universidade de São Paulo)
            </td>
        </tr> -->
        <!-- <tr>
            <td class="abstract">
                Here&rsquo;s where you put the abstract of the talk. Notice that with MathJax you can include mathematical notation in the abstract like \(f(x)=3x-7\) or like \(h^0(X_t,\omega_{X_t}^{\otimes m})\) or whatever you want. Just notice the MathJax line in the head of this HTML document that allows this. I&rsquo;m going to blab just a tiny bit more in this abstract, but all the other abstracts below, except the ones for the plenary talks, will be just <i>Lorem Ipsum</i> text that needs to be replaced by real abstracts.
            </td>
        </tr> -->
    <!-- </table>
    
    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="4">
                Jun 4, 2021 <br> 10:00 AM (GMT -3:00)
            </td>
            <td class="title" style="color: #52739e;">
                Lecture 7: Explaining Neural Scaling Laws (English) - <a href="https://www.youtube.com/watch?v=A8F4Qga3NaM&ab_channel=DataICMC">Recording link</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Jaehoon Lee (Google Brain)
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                For a large variety of models and datasets, neural network performance has been empirically observed to scale as a power-law with model size and dataset size. We would like to understand why these power laws emerge, and what features of the data and models determine the values of the power-law exponents. Since these exponents determine how quickly performance improves with more data and larger models, they are of great importance when considering whether to scale up existing models. In this talk, we’ll survey some of the well-known power-law scaling behavior observed in deep neural networks. Drawing intuition from statistical physics, we observe that a simplifying limit arises as one scales up deep learning models. We’ll talk about a theoretical framework that explains and connects various scaling laws. We identify variance-limited and resolution-limited scaling behavior for both dataset and model size, for a total of four scaling regimes.
            </td>
        </tr>
        <tr>
            <td>
                <a onclick="toggleText('bio-speaker-7')" >Show / Hide Biography</a>
                <div class='bio' id="bio-speaker-7" style="display: none;">
                    <b>Bio:</b> Jaehoon Lee is a senior research scientist at Google Brain team. His main research focus is on fundamental understanding of deep neural networks; actively working on the infinite-width limit of neural networks and their correspondence to the kernel methods. In 2017, he joined Google and started a research career in machine learning as part of the Google Brain Residency program. Before that he was a postdoctoral fellow at University of British Columbia from 2015-2017 working on theoretical high energy physics. Jaehoon obtained his PhD in physics at the Center for Theoretical Physics, Massachusetts Institute of Technology (MIT) in 2015.
                </div>
            </td>
        </tr>
    </table> -->


    <!-- <b>Week 8</b>
    <hr class="divider">
    <table>
        <tr>
            <td class="date" rowspan="3">
                Jun 7, 2021 <br> 06:00 PM (GMT -3:00)
            </td>
            <td class="title" style="color: #b2132e;"> 
                Introductory lecture 8: Introduction to Information Theory (Portuguese) - <a href="https://youtu.be/PRTlwr6AJyI">Recording link</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Eduarda Chagas (Universidade Federal de Minas Gerais)
            </td>
        </tr> -->
        <!-- <tr>
            <td class="abstract">
                Here&rsquo;s where you put the abstract of the talk. Notice that with MathJax you can include mathematical notation in the abstract like \(f(x)=3x-7\) or like \(h^0(X_t,\omega_{X_t}^{\otimes m})\) or whatever you want. Just notice the MathJax line in the head of this HTML document that allows this. I&rsquo;m going to blab just a tiny bit more in this abstract, but all the other abstracts below, except the ones for the plenary talks, will be just <i>Lorem Ipsum</i> text that needs to be replaced by real abstracts.
            </td>
        </tr> -->
    <!-- </table>
    
    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="4">
                Jun 11, 2021 <br> 02:00 PM (GMT -3:00)
            </td>
            <td class="title" style="color: #52739e;">
                Lecture 8: Progress Towards Understanding Generalization in Deep Learning (English) - <a href="https://youtu.be/Et_v-EIxqUs">Recording link</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Gintare Karolina Dziugaite (Element AI)
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                There is, as yet, no satisfying theory explaining why common learning algorithms, like those based on stochastic gradient descent, generalize in practice on overparameterized neural networks. I will discuss various approaches that have been taken to explaining generalization in deep learning, and identify some of the barriers these approaches faced. I will then discuss my recent work on information-theoretic and PAC-Bayesian approaches to understanding generalization in noisy variants of SGD. In particular, I will highlight how we can take advantage of conditioning to obtain sharper data- and distribution-dependent generalization measures. I will also briefly touch upon my work on properties of the optimization landscape and some of the challenges we face incorporating these insights into the theory of generalization.
            </td>
        </tr>
        <tr>
            <td>
                <a onclick="toggleText('bio-speaker-8')" >Show / Hide Biography</a>
                <div class='bio' id="bio-speaker-8" style="display: none;">
                    <b>Bio:</b> Gintare Karolina Dziugaite is a Lead Research Scientist at Element AI, a ServiceNow company. She is also an associate member at Mila, the Quebec AI Institute. Her research combines theoretical and empirical approaches to understanding deep learning, with a focus on generalization and network compression. Before joining Element AI, she obtained her Ph.D. in machine learning from the University of Cambridge, under the supervision of Zoubin Ghahramani. Prior to that, she studied Mathematics at the University of Warwick and read Part III in Mathematics at the University of Cambridge, receiving a Masters of Advanced Study (MASt) in Applied Mathematics. In 2020, Karolina was a member of the Institute for Advanced Study,, participating in the special year on Optimization, Statistics, and Theoretical Machine Learning. In 2019, she was a Simons Fellow during the Foundations of Deep Learning program at the Simons Institute for the Theory of Computing at the University of Berkeley from programs. She was also a long-term participant at the Simons Institute in 2017 and 2020 during programs on theoretical machine learning and interpretable machine learning.
                </div>
            </td>
        </tr>
    </table>


    <b>Week 9</b>
    <hr class="divider">
    
    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="4">
                Jun 18, 2021 <br> 01:30 PM (GMT -3:00)
            </td>
            <td class="title" style="color: #52739e;">
                Lecture 9: Information-Theoretic Generalization Bounds for Stochastic Gradient Descent (English) - <a href="https://www.youtube.com/watch?v=jJrOLPZJxSI">Recording link</a> 
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Gergely Neu (Universitat Pompeu Fabra)
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                We study the generalization properties of the popular stochastic gradient descent method for optimizing general non-convex loss functions. Our main contribution is providing upper bounds on the generalization error that depend on local statistics of the stochastic gradients evaluated along the path of iterates calculated by SGD. The key factors our bounds depend on are the variance of the gradients (with respect to the data distribution) and the local smoothness of the objective function along the SGD path, and the sensitivity of the loss function to perturbations to the final output. Our key technical tool is combining the information-theoretic generalization bounds previously used for analyzing randomized variants of SGD with a perturbation analysis of the iterates.
            </td>
        </tr>
        <tr>
            <td>
                <a onclick="toggleText('bio-speaker-9')" >Show / Hide Biography</a>
                <div class='bio' id="bio-speaker-9" style="display: none;">
                    <b>Bio:</b> Gergely Neu is a research assistant professor at the Pompeu Fabra University,
                    Barcelona, Spain. He has previously worked with the SequeL team of INRIA Lille,
                    France and the RLAI group at the University of Alberta, Edmonton, Canada. He
                    obtained his PhD degree in 2013 from the Budapest University of Technology and
                    Economics, where his advisors were András György, Csaba Szepesvári and László
                    Györfi. His main research interests are in machine learning theory, including
                    reinforcement learning and online learning with limited feedback and/or very
                    large action sets. Dr. Neu was the recipient of a Google Faculty Research award
                    in 2018, the Bosch Young AI Researcher Award in 2019, and an ERC Starting
                    Grant in 2020.
                </div>
            </td>
        </tr>
    </table> -->


    <!-- <b>Week 10</b>
    <hr class="divider">
    <table>
        <tr>
            <td class="date" rowspan="3">
                Jun 21, 2021 <br> 04:00 PM (GMT -3:00)
            </td>
            <td class="title" style="color: #b2132e;">
                Introductory lecture 10: Introduction to Category Theory: Up to Monoidal Categories (Portuguese) - <a href="https://www.youtube.com/watch?v=7ypPI7jHOTA">Recording link</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Jose Vitor Paiva Miranda Siqueira (University of Cambridge)
            </td>
        </tr> -->
        <!-- <tr>
            <td class="abstract">
                Here&rsquo;s where you put the abstract of the talk. Notice that with MathJax you can include mathematical notation in the abstract like \(f(x)=3x-7\) or like \(h^0(X_t,\omega_{X_t}^{\otimes m})\) or whatever you want. Just notice the MathJax line in the head of this HTML document that allows this. I&rsquo;m going to blab just a tiny bit more in this abstract, but all the other abstracts below, except the ones for the plenary talks, will be just <i>Lorem Ipsum</i> text that needs to be replaced by real abstracts.
            </td>
        </tr> -->
    <!-- </table>
    
    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="4">
                
                Jun 25, 2021 <br> 04:00 PM (GMT -3:00)

            </td>
            <td class="title" style="color: #52739e;">
                Lecture 10: Backprop as a Functor (English) - <a href="https://www.youtube.com/watch?v=N9zZeACcV98">Recording link</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Brendan Fong (MIT / Topos Institute)
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                A supervised learning algorithm searches over a set of functions \(A→B\) parametrised by a space \(P\) to find the best approximation to some ideal function \(f:A→B\). It does this by taking examples \((a,f(a))∈A×B\), and updating the parameter according to some rule. We define a category where these update rules may be composed, and show that gradient descent --- with respect to a fixed step size and an error function satisfying a certain property --- defines a monoidal functor from a category of parametrised functions to this category of update rules. This provides a structural perspective on backpropagation, as well as a broad generalisation of neural networks.
            </td>
        </tr>
        <tr>
            <td>
                <a onclick="toggleText('bio-speaker-10')" >Show / Hide Biography</a>
                <div class='bio' id="bio-speaker-10" style="display: none;">
                    <b>Bio:</b> Brendan Fong oversees coordination and strategic planning at Topos. He holds a PhD in computer science from Oxford, and undertook postdoctoral studies in systems engineering and mathematics at UPenn and MIT. He is a founding executive editor of the open-access journal Compositionality, and co-authored the textbook An Invitation to Applied Category Theory. Brendan believes technologies of connection and integration are already transforming the world we live in, and is dedicated to ensuring that these transformations benefit society-at-large.
                </div>
            </td>
        </tr>
    </table>


    <b>Week 11</b>
    <hr class="divider"> -->
    <!-- <table>
        <tr>
            <td class="date" rowspan="3">
                Jun 28, 2021 <br> Time yet to be defined
            </td>
            <td class="title" style="color: #b2132e;">
                Introductory lecture 11: (Portuguese)
            </td>
        </tr>
        <tr>
            <td class="speaker">
                To be defined
            </td>
        </tr>

    </table> -->
    
    <!-- <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="4">
                
                Jul 2, 2021 <br> 02:00 PM (GMT -3:00)

            </td>
            <td class="title" style="color: #52739e;">
                Lecture 11: Learning Functors using Gradient Descent (English) - <a href="https://www.youtube.com/watch?v=SQqzs3uAqGM">Recording link</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Bruno Gavranović (University of Strathclyde)
            </td>
        </tr>
        
        <tr>
            <td class="abstract">
                CycleGAN is a general approach to unpaired image-to-image translation that has been getting attention in recent years. Inspired by categorical database systems, we show that CycleGAN is a "schema", i.e. a specific category presented by generators and relations, whose specific parameter instantiations are just set-valued functors on this schema. We show that enforcing cycle-consistencies amounts to enforcing composition invariants in this category. We generalize the learning procedure to arbitrary such categories and show that a special class of functors, rather than functions, can be learned using gradient descent. Using this framework we design a novel neural network system capable of learning to insert and delete objects from images without paired data. We qualitatively evaluate the system on three different datasets and obtain promising results.

            </td>
        </tr>
        <tr>
            <td>
                <a onclick="toggleText('bio-speaker-11')" >Show / Hide Biography</a>
                <div class='bio' id="bio-speaker-11" style="display: none;">
                    <b>Bio:</b> Bruno Gavranović is a PhD student at the Mathematically Structured Programming (MSP) group in Glasgow, advised by Neil Ghani. He is  understanding machine learning and game theory through the lens of category theory and functional programming. Before joining MSP, he obtained his Bachelors and Masters at Faculty of Electrical Engineering and Computing (FER) in Zagreb.
                </div>
            </td>
        </tr>
    </table>


    <table class="plenary">
        <tr>
            <td class="date" rowspan="3">
                Jul 09, 2021 <br> 11:00 AM (GMT -3:00)
            </td>
            <td class="title" style="color: #52739e">
                Panel: The Many Paths to Understanding Deep Learning (English) - <a href="https://www.youtube.com/watch?v=NfvYufQwA_o">Recording link link</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Brendan Fong (MIT / Topos Institute), Gintare Karolina (Element AI), Oriol Vinyals (DeepMind), Yasaman Bahri (Google Research)
            </td>
        </tr>
        <tr> -->
            <!-- <td class="abstract">
                Here&rsquo;s the abstract for the plenary talk! Notice again that the formatting of this time-block is a bit different that the rest of of the talks.
                Filler text: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum dignissim et est et euismod. Fusce et metus tempus, pellentesque ex at, convallis nulla. Ut fringilla commodo tincidunt. Fusce sed est eu massa placerat iaculis eu at mauris. Nullam ut mollis nisi, quis malesuada risus. Interdum et malesuada fames ac ante ipsum primis in faucibus. Nam ipsum tortor, suscipit non tincidunt vel, bibendum in libero. Nulla facilisi. Pellentesque vitae neque metus. Cras quis est pharetra, vestibulum nisl et, viverra ipsum. Etiam porta dignissim purus, quis tempor metus volutpat eu. Praesent pulvinar libero eget purus tincidunt finibus.
            </td> -->
        <!-- </tr>
    </table> -->

    <table class="footer">
        <tr>
            <td class="footer">Design by <a target="_blank" href="http://math.ucr.edu/~mpierce/">Mike Pierce</a></td> 
            <!-- <td class="footer">&copy; Data ICMC</td>  -->
        </tr>
    </table>


</body>

</html>

